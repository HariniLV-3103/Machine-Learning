# Assignment 5 - Perceptron vs Multilayer Perceptron (A/B Experiment) with Hyperparameter Tuning  

## ðŸŽ¯ Aim
To implement and compare the performance of:
- **Model A:** Single-Layer Perceptron Learning Algorithm (PLA).  
- **Model B:** Multilayer Perceptron (MLP) with hidden layers and nonlinear activations.  

---

## ðŸ› ï¸ Libraries Used
- **NumPy** â€“ Numerical computations and array operations  
- **Pandas** â€“ Dataset handling and preprocessing  
- **Matplotlib** â€“ Data visualization  
- **Seaborn** â€“ Advanced visualization and heatmaps  
- **Scikit-learn** â€“ Preprocessing, evaluation metrics, model utilities  
- **TensorFlow/Keras** â€“ Building and training MLP models  
- **Pillow** â€“ Image loading and preprocessing  

---

## ðŸŽ¯ Objectives
- Implement PLA from scratch with step activation.  
- Build and train an MLP with hyperparameter tuning.  
- Compare both models on the **English Handwritten Characters dataset**.  

---

## ðŸ“– Theoretical Background
### ðŸ”¹ Perceptron Learning Algorithm (PLA)
- Weight update rule:  
  \[
  w^{t+1} = w^t + \eta (y - \hat{y}) x
  \]  
- Works only for linearly separable datasets.  

### ðŸ”¹ Multilayer Perceptron (MLP)
- Architecture: Input â†’ Hidden Layers â†’ Output  
- Uses nonlinear activations (ReLU, Sigmoid, Tanh)  
- Loss function: **Cross-Entropy** for classification  
- Optimizers: **SGD, Adam**  
- Learns nonlinear decision boundaries using **backpropagation**  

---

## ðŸ“‚ Dataset
- **Dataset:** English Handwritten Characters Dataset  
- **Samples:** 3,410 images  
- **Classes:** 62 (0â€“9, Aâ€“Z, aâ€“z)  
- **Preprocessing:** Resize (32Ã—32), flatten, normalize pixel values  

---

## ðŸ“ Implementation Steps
1. Preprocess dataset (resize, flatten, normalize).  
2. Implement **PLA** from scratch.  
3. Implement **MLP** with multiple configurations.  
4. Perform hyperparameter tuning (activation, optimizer, LR, batch size).  
5. Evaluate using Accuracy, Precision, Recall, F1, Confusion Matrix, ROC curves.  

---

## âš™ï¸ Hyperparameters
- **PLA:**  
  - Step activation  
  - Learning rate = 0.01  
  - Epochs = 30  

- **MLP:**  
  - 2 Hidden Layers (512, 256 neurons)  
  - Activation: ReLU (hidden), Softmax (output)  
  - Loss: Categorical Cross-Entropy  
  - Optimizer: Adam  
  - Learning Rate = 0.001  
  - Batch Size = 32  
  - Epochs = 25  

---

## ðŸ“Š Results
### ðŸ”¹ Perceptron (PLA)
- Test Accuracy: **17.7%**  
- Precision: 0.2708  
- Recall: 0.1774  
- F1-score: 0.1576  

### ðŸ”¹ Multilayer Perceptron (MLP - best config: ReLU + Adam, lr=0.001, batch=32)
- Test Accuracy: **29.8%**  
- Precision: 0.3207  
- Recall: 0.2977  
- F1-score: 0.2752  

---

## ðŸ“‰ Comparison
- PLA underperformed due to its **linear separability limitation**.  
- MLP outperformed PLA by learning **nonlinear decision boundaries**.  
- **Adam + ReLU** gave the best performance.  
- Batch size 32 generalized better than 64.  
- More hidden layers helped initially, but too many caused **diminishing returns**.  

---

## ðŸ”Ž Observations
- PLA failed for nonlinear data â†’ Accuracy only **17.7%**.  
- MLP achieved **29.8% accuracy**, showing better representational capacity.  
- **Optimizer choice** mattered: Adam >> SGD.  
- **Learning rate 0.001** stabilized training, while **0.01 diverged**.  
- No strong overfitting detected, but deeper models may need dropout/regularization.  

---

## âœ… Final Summary
| Model | Epochs | LR | Test Accuracy | Precision | Recall | F1-score |
|-------|--------|----|---------------|-----------|--------|----------|
| **PLA** | 30 | 0.01 | 0.1774 | 0.2708 | 0.1774 | 0.1576 |
| **MLP (best)** | 25 | 0.001 | 0.2977 | 0.3207 | 0.2977 | 0.2752 |

---

## ðŸ“Œ Conclusion
- PLA is insufficient for complex datasets with nonlinear class boundaries.  
- MLP demonstrates significant improvements, but requires **careful hyperparameter tuning**.  
- For further improvement: add dropout, batch normalization, or increase training epochs.  
